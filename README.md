# Prompt Evaluation Sandbox (LLM Judgment System)

This project simulates real-world prompt rating tasks like those used by Outlier AI and Surge AI. It evaluates LLM-generated responses for:

-  Factual Accuracy  
-  Clarity & Coherence  
-  Tone Appropriateness  

> This tool helps assess and train LLM behavior â€” a core skill for language model rater positions.


##  Key Features

- **Open-source Falcon-7B model integration**
- Real-time prompt generation and response scoring
- Interactive scoring interface for clarity, tone, and factuality
- Automatic CSV logging (`prompt_eval_log.csv`) for portfolio export
- HuggingFace + PyTorch + Transformers pipeline


## How to Run

```bash
# Clone the repo
git clone https://github.com/Tostage/prompt-eval-mistral.git
cd prompt-eval-mistral

# Set up environment
python -m venv venv
source venv/bin/activate  # (or venv\Scripts\activate on Windows)

# Install requirements
pip install -r requirements.txt

# Run evaluation loop
python test_prompt.py

##Tech Stack

-Falcon 1B via Hugging Face Transformers
-Python
-Streamlit (optional UI layer)
-CSV logging for evaluation output
