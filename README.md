# prompt-eval-mistral

This simulates LLM eval tasks: fact-checking, tone analysis, clarity scoring and prompt testing complete with a human in the loop scoring system and CSV export for QA workflows.
